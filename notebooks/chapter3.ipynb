{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For our purposes, it is convenient to place the boundary of the learning agent not at the limit of its physical body, but at the limit of its control.\"  - 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1\n",
    "\n",
    "##### Exercise 3.2\n",
    "\n",
    "Is the reinforcement learning framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n",
    "\n",
    "??\n",
    "\n",
    "##### Exercise 3.4\n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for  upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "For an episodic task with the above rewards, the discounted future reward would be exactly -1 * gamma^k, where k is the time step of failure, since that is when the episode ends. In the continuing formulation task, the reward is -1 * gamma^k1 + -1 * gamma^k2, etc. for each failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.6 - MDPs\n",
    "\n",
    "##### Exercise 3.7\n",
    "Assuming a finite MDP with a finite number of reward values, write an equation for the transition probabilities and the expected rewards in terms of the joint conditional distribution in (3.5) $Pr(s_{t+1} = s', r_{t+1} = r| s_t, a_t)$\n",
    "\n",
    "$P^a_{s,s'} = Pr(s_{t+1} = s' | s_t = s, a_t = a) = \\sum_r Pr(s_{t+1} = s', r_{t+1} = r| s_t, a_t) $\n",
    "\n",
    "$R^a_{s, s'} = E[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s'] = \\sum_r r * Pr(s_{t+1} = s', r_{t+1} = r| s_t, a_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.7 - Value Functions\n",
    "\n",
    "##### Exercise 3.8\n",
    "What is the Bellman equation for action values?\n",
    "\n",
    "$$ Q^{\\pi}(s, a) = E_{\\pi} [R_t | s_t = s, a_t = a] = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k r_{t + k + 1} | s_t = s, a_t = a ]$$\n",
    "$$= E_{\\pi} [r_{t + 1} + \\gamma \\sum_{k=0}^{\\infty} \\gamma^k r_{t + k + 2} | s_t = s, a_t = a]$$\n",
    "$$= \\sum_{s'} P^a_{ss'} R^a_{ss'} + \\sum_{s'} P^a_{ss'} \\gamma \\sum_{k=0}^{\\infty} \\gamma^k r_{t + k + 2}$$\n",
    "$$= \\sum_{s'} P^a_{ss'} [R^a_{ss'} + \\gamma V^{\\pi}(s')]$$\n",
    "$$= \\sum_{s'} P^a_{ss'} [R^a_{ss'} + \\gamma \\sum_{a'} \\pi(s', a') Q^{\\pi}(s', a')]$$\n",
    "\n",
    "\n",
    "##### Exercise 3.9\n",
    "The Bellman equation (3.10) must hold for each state for the value function  shown in Figure  3.5b. As an example, show numerically that this equation holds for the center state, valued at 0.7 ,\n",
    "\n",
    "$V^{\\pi}(s_{mid}) = \\frac{1}{4} [(0 + .9 * .7) + (0 + 0.9 * 2.3) + (0 + .9 * .4) + (0 - .9 * .4)] = .675 \\approx .7$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.10\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.2), that adding a constant $C$ to all the rewards adds a constant, $K$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $K$ in terms of $C$ and $\\gamma$?\n",
    "\n",
    "We have from previously that $V^{\\pi}(s) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} | s_t = s]$.\n",
    "\n",
    "Let's add a constant $C$ to all rewards so that $r'_{t+1} = r_{t+1} + C$.\n",
    "\n",
    "Then, $$V'^{\\pi}(s) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k (r_{t+k+1} + C) | s_t = s]$$\n",
    "$$ = V^{\\pi}(s) + E_{\\pi}[ \\sum_{k=0}^{\\infty} \\gamma^k C | s_t = s ]$$\n",
    "$$ = V^{\\pi}(s) + K $$\n",
    "\n",
    "We have, \n",
    "\n",
    "$$K = E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k C | s_t = s]$$\n",
    "$$ = \\sum_a \\pi(s, a) \\sum_s' P^a_{ss'} (C \\frac{1}{1 - \\gamma})$$\n",
    "$$ = \\frac{C}{1 - \\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.11 \n",
    "\n",
    "Now consider adding a constant C to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    " \n",
    "For episodic tasks we have,\n",
    "\n",
    "$$K = \\sum_a \\pi(s, a) \\sum_s' P^a_{ss'} \\sum_k^{N}(C \\gamma^k)$$\n",
    "\n",
    "where $N$ is the number of time steps until the episode is over. It seems like the reward is maximized as N goes to infiinity, so adding a constant reward to all actions in an episodic task makes the agent never want to end interacting with the environment to avoid finishing the episode.\n",
    "\n",
    "???\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.12\n",
    "\n",
    "The value of a state depends on the the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    " \n",
    "\n",
    "$$V^{\\pi} (s) = E_{\\pi} [Q^{\\pi} (s, a) | s_t = s] $$\n",
    "$$ = \\sum_a \\pi(s, a) Q^{\\pi} (s, a)$$\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3.13\n",
    "\n",
    "The value of an action, , can be divided into two parts, the expected next reward, which does not depend on the policy , and the expected sum of the remaining rewards, which depends on the next state and the policy. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state-action pair) and branching to the possible next states:\n",
    "\n",
    "$$ Q^{\\pi} (s, a) = E[ r_{t+1} | s_t  = s, a_t = a] + \\gamma E_{\\pi}[ V^{\\pi}(s') | s_t = s, a_t = a] $$\n",
    "\n",
    "$$ Q^{\\pi} (s, a) = \\sum_{s'} P^a_{ss'} [R_{ss'} + \\gamma V^{\\pi}(s')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 - Optimal Value Functions\n",
    "\n",
    "##### 3.16\n",
    "Give the Bellman equation for  for the recycling robot.\n",
    "s = search, w = wait, re = recharge\n",
    "h = high, l = low\n",
    "\n",
    "$Q^*(s, a) = \\sum_{s'} P^a_{s, s'} [R^a_{s,s'} + \\gamma max_{a'} Q^*(s', a')]$\n",
    "\n",
    "$Q^*(h, s) = \\alpha (R^s + \\gamma max_{a'} Q^*(h, a')) + (1 - \\alpha) (R^s + \\gamma max_{a'} Q^*(l, a'))$\n",
    "\n",
    "$Q^*(l, s) = \\beta (R^s + \\gamma max_{a'} Q^*(l, a')) + (1 - \\beta) (-3 + \\gamma max_{a'} Q^*(h, a'))$\n",
    "\n",
    "$Q^*(l, w) = (R^w + \\gamma max_{a'} Q^*(l, a'))$\n",
    "\n",
    "$Q^*(h, w) = (R^w + \\gamma max_{a'} Q^*(h, a'))$\n",
    "\n",
    "$Q^*(l, r) = (0 + \\gamma max_{a'} Q^*(h, a'))$\n",
    "\n",
    "$Q^*(h, r) = 0$\n",
    "\n",
    "\n",
    "##### 3.17 \n",
    "Figure  3.8 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.2) to express this value symbolically, and then to compute it to three decimal places.\n",
    "\n",
    "\n",
    "$V^*(s) = max_{a \\in A(s)} \\sum_{s'} P^a_{s,s'}[R^a_{s, s'} + \\gamma V^*(s')]$\n",
    "\n",
    "So we have,\n",
    "\n",
    "$$V^*(s) = max_a E[r_{t+1}  + \\gamma \\sum_{k=0}^{\\infty} \\gamma^k r_{t + k+2} | s_t=s, s_t = a]$$\n",
    "$$ = max_a [10 + \\gamma * 0 + \\gamma^2 * 0 + \\gamma^3 * 0 + \\gamma^4 + \\gamma^5 * 10 + ...]$$\n",
    "$$ = 10(\\sum_{k=0}^\\infty \\gamma^{5k})$$\n",
    "$$ = 10 \\frac{1}{1 - \\gamma^5} \\approx 24.419$$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
