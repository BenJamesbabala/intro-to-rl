{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5.1\n",
    "\n",
    "Consider the diagrams on the right in Figure  5.2. Why does the value function jump up for the last two rows in the rear? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?\n",
    "\n",
    "Jumps up for last two rows in the rear since sticking for 20 and 21 in blackjack usually result in a win. It drops off for the last row on the left since if the dealer has an Ace, it's bad news for the player. Finally, the frontmost values are higher in the upper diagrams than in the lower since having a usable Ace makes it less likely that an Ace will make the player bust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5.2\n",
    "\n",
    "The backup diagram for the Monte Carlo estimation of $Q^\\pi$ is similar to the backup diagram for $V^\\pi$, but the root is a state-action pair, not just a state. The diagram ends in a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some notes \n",
    "\"Without a model (as we had in DP chapter 4)... state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. \"\n",
    "\n",
    "By model, the author means, having the transition probabilities and transition rewards available at hand.\n",
    "\n",
    "\" For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the first step of each episode starts at a state-action pair, and that every such pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of exploring starts.\"\n",
    "\n",
    "\n",
    "Has the following been proved on Monte Carlo ES?\n",
    "\"Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, but has not yet been formally proved. In our opinion, this is one of the most fundamental open questions in reinforcement learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Questions\n",
    "\n",
    "**Question**: In 5.6, in the Figure 5.7, (c), the update to w, why is there a 1 in the numerator? It's because $\\pi(s,a)$ is a deterministic policy!\n",
    "\n",
    "Why are we taking $\\tau$ to be the latest time at which the actions are not greedy? Is it because our estimate for $Q^\\pi$ only improves for nongreedy actions as stated in the section?\n",
    "\n",
    "**Question**: In 5.4, the conditions for the policy improvement theorem require optimal substructure right? So even though Monte Carlo is more robust to violations of the Markov Property since it doesn't bootstrap $V$ or $Q$, we are still assuming that greedy updates in GPI (Generalized Policy Improvement) will allow us to arrive at the optimal action-value functions due to the Markov Property?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5.3 \n",
    "\n",
    "What is the Monte Carlo estimate analogous to (5.3) for action values, given returns generated using $\\pi'$?\n",
    "\n",
    "I'm not sure, but I think it would be something like:\n",
    "\n",
    "$$Q(s,a) = \\frac{\\sum_i^{n_s} \\frac{p_i(s,a)}{p_i'(s,a)} R_i(s,a) }{ \\sum_i^{n_s} \\frac{p_i(s,a)}{p_i'(s,a)} }$$\n",
    "\n",
    "Where $R_i(s,a)$ is the reward following state $s$ and action $a$, and $p_i(s_t, a_t) = \\prod_{k=t}^{T_i(s) - 2} \\pi(s_k, a_k)P^a_{s_k, s_{k+1}} \\cdot \\pi(s_{T_i(s) - 1}, a_{T_i(s) - 1}) $. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Track(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            0 = off track\n",
    "            1 = road - on track\n",
    "            2 = start line\n",
    "            3 = finish line\n",
    "        \"\"\"\n",
    "        self.track = np.ones((18, 18))\n",
    "        self.track[5:18, 5:18] = 0\n",
    "        self.track[-1, :] *= 2\n",
    "        self.track[:, -1] *= 3\n",
    "\n",
    "    def get_next_position(self, racecar):\n",
    "        \"\"\"\n",
    "            RaceCar racecar: RaceCar object\n",
    "        \"\"\"\n",
    "        \n",
    "        reward = -1\n",
    "        \n",
    "#         print('starting position is ', (racecar.x, racecar.y))\n",
    "#         print('starting velocity is ', (racecar.velocity_x, racecar.velocity_y))\n",
    "        \n",
    "        new_x = racecar.x + racecar.velocity_x\n",
    "        new_y = racecar.y + racecar.velocity_y\n",
    "        \n",
    "        final_x = new_x\n",
    "        final_y = new_y\n",
    "        \n",
    "        # Compute all the unique boxes we hit on a line between the start and end points\n",
    "        x_positions = np.linspace(racecar.x, new_x, num=20)\n",
    "        y_positions = np.linspace(racecar.y, new_y, num=20)\n",
    "        positions = zip(x_positions, y_positions)\n",
    "        positions = [(np.floor(x), np.floor(y)) for x, y in positions]\n",
    "        \n",
    "        # Get unique discrete positions visited during this time step\n",
    "        ordered_positions = []\n",
    "        for pos in positions:\n",
    "            if len(ordered_positions) == 0 or pos != ordered_positions[-1]:\n",
    "                ordered_positions.append(pos)\n",
    "                        \n",
    "        # Check if the car crashes into the track at any of those time points\n",
    "        for pos_idx, pos in enumerate(ordered_positions):\n",
    "            \n",
    "            # speed past the finish without penalty?\n",
    "            if self.is_terminal_state_from_coordinates(pos[0], pos[1]):\n",
    "                reward = -1\n",
    "                final_x, final_y = ordered_positions[pos_idx]\n",
    "                break\n",
    "\n",
    "            if self.is_out_of_bounds(pos):\n",
    "#                 print('CRASH')\n",
    "                reward = -5\n",
    "                crash_x, crash_y = pos\n",
    "                final_x, final_y = ordered_positions[pos_idx - 1]\n",
    "                racecar.velocity_x = 0\n",
    "                racecar.velocity_y = 0\n",
    "                break\n",
    "\n",
    "        # If the car is not moving, move at least 1 step\n",
    "        if final_x == racecar.x and final_y == racecar.y:\n",
    "            if self.is_out_of_bounds((final_x + 1, final_y)):\n",
    "                final_y += 1\n",
    "                racecar.velocity_y = 1\n",
    "            elif self.is_out_of_bounds((final_x, final_y + 1)):\n",
    "                final_x += 1\n",
    "                racecar.velocity_x = 1\n",
    "            else:\n",
    "                random_choice = np.random.choice([0, 1])\n",
    "                final_x += random_choice\n",
    "                final_y += (1 - random_choice)\n",
    "                racecar.velocity_x += random_choice\n",
    "                racecar.velocity_y += (1 - random_choice)                    \n",
    "        \n",
    "#         print('update position is ', (final_x, final_y))\n",
    "        \n",
    "        racecar.x = final_x\n",
    "        racecar.y = final_y\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def convert_cartesian_to_indexes(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        x_prime = self.track.shape[0] - x_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def convert_indexes_to_cartesian(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        y_prime = self.track.shape[1] - y_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def is_terminal_state(self, racecar):\n",
    "        x, y = self.convert_cartesian_to_indexes(racecar.x, racecar.y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_terminal_state_from_coordinates(self, x, y):\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_out_of_bounds(self, position):\n",
    "        x, y = position\n",
    "        \n",
    "        if x < 0 or x >= self.track.shape[1]:\n",
    "            return True\n",
    "        \n",
    "        if y < 0 or y >= self.track.shape[0]:\n",
    "            return True\n",
    "\n",
    "        # y is reversed in our frame of reference\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "\n",
    "        if self.track[x, y] == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_random_start(self):\n",
    "        # returns x and y coordinates of random start\n",
    "        starts = np.argwhere(self.track == 2)\n",
    "        random_start = np.random.randint(len(starts))\n",
    "        start = starts[random_start]\n",
    "        return self.convert_indexes_to_cartesian(*start)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return [self.convert_indexes_to_cartesian(x, y) for x, y in np.argwhere(self.track != 0)]\n",
    "    \n",
    "    def print_track(self, x, y):\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        pt = np.copy(self.track)\n",
    "        pt[x, y] = -1\n",
    "        print(pt)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RaceCar(object):\n",
    "    def __init__(self):\n",
    "        self.velocity_x = 0\n",
    "        self.velocity_y = 0\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "        self.MAX_VELOCITY = 5\n",
    "        self.MIN_VELOCITY = 0\n",
    "\n",
    "    def get_episode(self, pi, track, actions, states, greedy=False, verbose=False):\n",
    "        \"\"\"\n",
    "            actions: an index to action dictionary\n",
    "        \n",
    "        \"\"\"\n",
    "        self.velocity_x = 0; self.velocity_y = 0\n",
    "        self.x, self.y = track.get_random_start()\n",
    "        rewards = []\n",
    "        visited_states = [((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "        saved_actions = []\n",
    "        \n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            state_idx = states[((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "\n",
    "            # Choose action according to PI\n",
    "            if greedy:\n",
    "                action_idx = np.where(pi[state_idx, :] == np.amax(pi[state_idx, :]))[0]\n",
    "                action_idx = np.random.choice(action_idx)   \n",
    "            else:\n",
    "                action_idx = np.random.choice(len(actions), size=1, p=pi[state_idx, :])[0]    \n",
    "            \n",
    "            action = actions[action_idx]\n",
    "            saved_actions.append(action)\n",
    "            \n",
    "            # Take the action\n",
    "            self.velocity_x += action[0]\n",
    "            self.velocity_y += action[1]\n",
    "\n",
    "            self.velocity_x = min(max(self.velocity_x, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "            self.velocity_y = min(max(self.velocity_y, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "\n",
    "            # save the rewards and states\n",
    "            reward = track.get_next_position(self)\n",
    "            if len(visited_states) > 100:\n",
    "                reward = -1000\n",
    "                terminated = True\n",
    "            else:\n",
    "                terminated = track.is_terminal_state(self)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            visited_states.append(((self.x, self.y), (self.velocity_x, self.velocity_y)))\n",
    "            \n",
    "            if verbose:\n",
    "                track.print_track(self.x, self.y)\n",
    "                print('Velocity is now: ', (self.velocity_x, self.velocity_y))\n",
    "        \n",
    "        return visited_states, saved_actions, rewards\n",
    "        \n",
    "    def get_velocity_states(self):\n",
    "        return list(product(\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1),\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car = RaceCar()\n",
    "track = Track()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On Policy Monte Carlo for the Race Track problem\n",
    "actions_list = list(product([-1, 0, 1], [-1, 0, 1]))\n",
    "actions_to_idx = {action: idx for idx, action in enumerate(actions_list)}\n",
    "idx_to_actions = {idx: action for idx, action in enumerate(actions_list)}\n",
    "\n",
    "states_list = list(product(track.get_states(), car.get_velocity_states()))\n",
    "states_to_idx = {state: idx for idx, state in enumerate(states_list)}\n",
    "\n",
    "Q = np.random.random((len(states_to_idx), len(actions_to_idx)))\n",
    "Returns = {(s, a): [] for s, a in product(states_to_idx, actions_to_idx)}\n",
    "\n",
    "pi = np.random.random((len(states_to_idx), len(actions_to_idx)))\n",
    "pi = pi / np.sum(pi, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On Policy Monte Carlo\n",
    "\n",
    "count = 0\n",
    "iamhappy = True\n",
    "epsilon = 0.1\n",
    "    \n",
    "while iamhappy:\n",
    "\n",
    "    visited_states, actions_taken, rewards = car.get_episode(pi, track, idx_to_actions, states_to_idx)\n",
    "    \n",
    "    has_visited_first_occurence = {}\n",
    "    for idx, sa in enumerate(zip(visited_states, actions_taken)):\n",
    "        s, a = sa\n",
    "        if (s, a) not in has_visited_first_occurence:\n",
    "            Returns[(s, a)].append(sum(rewards[idx:]))\n",
    "            Q[states_to_idx[s], actions_to_idx[a]] = np.mean(Returns[(s, a)]) \n",
    "            has_visited_first_occurence[(s, a)] = 0\n",
    "    \n",
    "    for s in visited_states:\n",
    "        # We can take the greedy action, but it's probably better to break ties\n",
    "        # a_star = np.argmax(Q[states_to_idx[s],:])\n",
    "        action_idx = np.where(Q[states_to_idx[s],:] == np.amax(Q[states_to_idx[s],:]))[0]\n",
    "        a_star = np.random.choice(action_idx)\n",
    "        for action_idx, a in enumerate(actions_list):\n",
    "            if a_star == action_idx:\n",
    "                pi[states_to_idx[s], action_idx] = 1 - epsilon + epsilon / len(actions_list)\n",
    "            else:\n",
    "                pi[states_to_idx[s], action_idx] = epsilon / len(actions_list)\n",
    "            \n",
    "    count += 1\n",
    "    \n",
    "    if count >= 4000: iamhappy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (0, 1))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (0, 1))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (0, 2))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (0, 3))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1. -1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (1, 2))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (2, 3))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (1, 2))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (2, 2))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (3, 1))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (4, 0))\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "('Velocity is now: ', (4, 0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([((1, 0), (0, 0)),\n",
       "  ((1, 1), (0, 1)),\n",
       "  ((1, 2), (0, 1)),\n",
       "  ((1, 4), (0, 2)),\n",
       "  ((1, 7), (0, 3)),\n",
       "  ((2, 9), (1, 2)),\n",
       "  ((4, 12), (2, 3)),\n",
       "  ((5, 14), (1, 2)),\n",
       "  ((7, 16), (2, 2)),\n",
       "  ((10, 17), (3, 1)),\n",
       "  ((14, 17), (4, 0)),\n",
       "  ((17.0, 17.0), (4, 0))],\n",
       " [(-1, 1),\n",
       "  (-1, 0),\n",
       "  (0, 1),\n",
       "  (-1, 1),\n",
       "  (1, -1),\n",
       "  (1, 1),\n",
       "  (-1, -1),\n",
       "  (1, 0),\n",
       "  (1, -1),\n",
       "  (1, -1),\n",
       "  (0, -1)],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.get_episode(pi, track, idx_to_actions, states_to_idx, greedy=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class MonteCarloSolver(object):\n",
    "#     def __init__(self, actions, states, agent, environment):\n",
    "#         self.Q = np.random.random((len(states), len(actions)))\n",
    "#         self.Returns = np.zeros((len(states), len(actions)))\n",
    "\n",
    "#         self.pi = np.random.random((len(states), len(actions)))\n",
    "#         self.pi = self.pi / np.sum(self.pi, axis=1)[:, None]\n",
    "        \n",
    "#         self.agent = agent\n",
    "#         self.environment = environment\n",
    "\n",
    "#     def solve(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5.5\n",
    "\n",
    "Modify the algorithm for first-visit MC policy evaluation (Figure  5.1) to use the incremental implementation for stationary averages described in Section 2.5.\n",
    "\n",
    "We just need to update the algorithm so that $Returns(s)$ is a 1x1 array for each $s \\in S$. Before part(b) of the algorithm, we need to initialize $k = 0$, and in part (b) in the loop, we do:\n",
    "\n",
    "- $Returns(s) \\leftarrow Returns(s) + \\frac{1}{k + 1}[R - Returns(s)]$ \n",
    "- $k \\leftarrow k + 1$\n",
    "\n",
    "\n",
    "##### Exercise 5.6\n",
    "\n",
    "\n",
    "Derive the weighted-average update rule (5.5) from (5.4). Follow the pattern of the derivation of the unweighted rule (2.4) from (2.1).\n",
    "\n",
    "We have, \n",
    "\n",
    "$$V_{n+1} = \\frac{\\sum_{k=1}^{n+1} w_k R_k }{\\sum{k+1}{n+1} w_k}$$\n",
    "$$= \\frac{w_{n+1} R_{n+1} + \\sum_{k=1}^n w_k R_k }{W_{n+1}}$$\n",
    "\n",
    "where $W_{n+1} = W_n + w_{n+1}$ and $W_0 = 0$. Then we have:\n",
    "\n",
    "$$V_{n+1} = \\frac{1}{W_{n+1}} [w_{n+1}R_{n+1} + V_n W_n] $$\n",
    "$$ = \\frac{1}{W_{n+1}} [w_{n+1}R_{n+1} + V_n (W_{n+1} - w_{n+1})] $$\n",
    "$$ = V_n +  \\frac{w_{n+1}}{W_{n+1}} [R_{n+1} - V_n ] .$$\n",
    "\n",
    "\n",
    "##### Exercise 5.7\n",
    "\n",
    "Modify the algorithm for the off-policy Monte Carlo control algorithm (Figure  5.7) to use the method described above for incrementally computing weighted averages.\n",
    "\n",
    "Before the repeat loop, we need to initialize $W = 0$. Then in the repeat loop we do:\n",
    "\n",
    "- get $w$ and $t$ as usual\n",
    "- delete lines for $N(s,a)$ and $D(s,a)$\n",
    "- $W \\leftarrow w + W$\n",
    "- $Q(s,a) \\leftarrow Q(s,a) + \\frac{w}{W} [R_t - Q(s,a)] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
