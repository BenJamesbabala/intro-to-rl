{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Track(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            0 = off track\n",
    "            1 = road - on track\n",
    "            2 = start line\n",
    "            3 = finish line\n",
    "        \"\"\"\n",
    "        self.track = np.ones((5, 5))\n",
    "        self.track[3:5, 3:5] = 0\n",
    "        self.track[-1, :] *= 2\n",
    "        self.track[:, -1] *= 3\n",
    "\n",
    "    def get_next_position(self, racecar):\n",
    "        \"\"\"\n",
    "            RaceCar racecar: RaceCar object\n",
    "        \"\"\"\n",
    "        \n",
    "        reward = -1\n",
    "        \n",
    "        new_x = racecar.x + racecar.velocity_x\n",
    "        new_y = racecar.y + racecar.velocity_y\n",
    "        \n",
    "        final_x = new_x\n",
    "        final_y = new_y\n",
    "        \n",
    "        # Compute all the unique boxes we hit on a line between the start and end points\n",
    "        x_positions = np.linspace(racecar.x, new_x, num=20)\n",
    "        y_positions = np.linspace(racecar.y, new_y, num=20)\n",
    "        positions = zip(x_positions, y_positions)\n",
    "        positions = [(np.floor(x), np.floor(y)) for x, y in positions]\n",
    "        \n",
    "        # Get unique discrete positions visited during this time step\n",
    "        ordered_positions = []\n",
    "        for pos in positions:\n",
    "            if len(ordered_positions) == 0 or pos != ordered_positions[-1]:\n",
    "                ordered_positions.append(pos)\n",
    "                        \n",
    "        # Check if the car crashes into the track at any of those time points\n",
    "        #   or if it reached the finish line\n",
    "        for pos_idx, pos in enumerate(ordered_positions):\n",
    "            \n",
    "            # ability to speed past the finish without penalty\n",
    "            if self.is_terminal_state_from_coordinates(pos[0], pos[1]):\n",
    "                reward = -1\n",
    "                final_x, final_y = ordered_positions[pos_idx]\n",
    "                break\n",
    "            \n",
    "            # check if the car crashes\n",
    "            if self.is_out_of_bounds(pos):\n",
    "                reward = -5\n",
    "                crash_x, crash_y = pos\n",
    "                final_x, final_y = ordered_positions[pos_idx - 1]\n",
    "                racecar.velocity_x = 0\n",
    "                racecar.velocity_y = 0\n",
    "                break\n",
    "\n",
    "        # If the car is not moving, the car must move at least 1 step\n",
    "        if final_x == racecar.x and final_y == racecar.y:\n",
    "            if self.is_out_of_bounds((final_x + 1, final_y)):\n",
    "                final_y += 1\n",
    "                racecar.velocity_y = 1\n",
    "            elif self.is_out_of_bounds((final_x, final_y + 1)):\n",
    "                final_x += 1\n",
    "                racecar.velocity_x = 1\n",
    "            else:\n",
    "                random_choice = np.random.choice([0, 1])\n",
    "                final_x += random_choice\n",
    "                final_y += (1 - random_choice)\n",
    "                racecar.velocity_x += random_choice\n",
    "                racecar.velocity_y += (1 - random_choice)                    \n",
    "        \n",
    "        racecar.x = final_x\n",
    "        racecar.y = final_y\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def convert_cartesian_to_indexes(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        x_prime = self.track.shape[0] - x_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def convert_indexes_to_cartesian(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        y_prime = self.track.shape[1] - y_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def is_terminal_state(self, racecar):\n",
    "        x, y = self.convert_cartesian_to_indexes(racecar.x, racecar.y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_terminal_state_from_coordinates(self, x, y):\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_out_of_bounds(self, position):\n",
    "        x, y = position\n",
    "        \n",
    "        if x < 0 or x >= self.track.shape[1]:\n",
    "            return True\n",
    "        \n",
    "        if y < 0 or y >= self.track.shape[0]:\n",
    "            return True\n",
    "\n",
    "        # y is reversed in our frame of reference\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "\n",
    "        if self.track[x, y] == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_random_start(self):\n",
    "        # returns x and y coordinates of random start\n",
    "        starts = np.argwhere(self.track == 2)\n",
    "        random_start = np.random.randint(len(starts))\n",
    "        start = starts[random_start]\n",
    "        return self.convert_indexes_to_cartesian(*start)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return [self.convert_indexes_to_cartesian(x, y) for x, y in np.argwhere(self.track != 0)]\n",
    "    \n",
    "    def print_track(self, x, y):\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        pt = np.copy(self.track)\n",
    "        pt[x, y] = -1\n",
    "        print(pt)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RaceCar(object):\n",
    "    def __init__(self):\n",
    "        self.velocity_x = 0\n",
    "        self.velocity_y = 0\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "        self.MAX_VELOCITY = 5\n",
    "        self.MIN_VELOCITY = 0\n",
    "\n",
    "    def get_episode(self, pi, track, actions, states, greedy=False, verbose=False):\n",
    "        \"\"\"\n",
    "            actions: an index to action dictionary\n",
    "            pi: numpy array of probabilities to take an action given the state\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.velocity_x = 0; self.velocity_y = 0\n",
    "        self.x, self.y = track.get_random_start()\n",
    "        \n",
    "        rewards = []\n",
    "        saved_actions = []\n",
    "        visited_states = [((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "        \n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            state_idx = states[((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "\n",
    "            # choose greedy action of action with probability pi\n",
    "            if greedy:\n",
    "                action_idx = np.where(pi[state_idx, :] == np.amax(pi[state_idx, :]))[0]\n",
    "                action_idx = np.random.choice(action_idx)   \n",
    "            else:\n",
    "                action_idx = np.random.choice(len(actions), size=1, p=pi[state_idx, :])[0]    \n",
    "            \n",
    "            action = actions[action_idx]\n",
    "            saved_actions.append(action)\n",
    "            \n",
    "            # Take the action\n",
    "            self.velocity_x += action[0]\n",
    "            self.velocity_y += action[1]\n",
    "            self.velocity_x = min(max(self.velocity_x, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "            self.velocity_y = min(max(self.velocity_y, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "\n",
    "            # save the rewards and states\n",
    "            reward = track.get_next_position(self)\n",
    "            if len(visited_states) > 100:\n",
    "                reward = -1000\n",
    "                terminated = True\n",
    "            else:\n",
    "                terminated = track.is_terminal_state(self)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            visited_states.append(((self.x, self.y), (self.velocity_x, self.velocity_y)))\n",
    "            \n",
    "            if verbose:\n",
    "                track.print_track(self.x, self.y)\n",
    "                print('Velocity is now: ', (self.velocity_x, self.velocity_y))\n",
    "        \n",
    "        return visited_states, saved_actions, rewards\n",
    "        \n",
    "    def get_states(self):\n",
    "        return list(product(\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1),\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MonteCarlo(object):\n",
    "    def __init__(self, actions, agent, environment):\n",
    "        self.actions_list = actions\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "        \n",
    "        self.actions_to_idx = {action: idx for idx, action in enumerate(self.actions_list)}\n",
    "        self.idx_to_actions = {idx: action for idx, action in enumerate(self.actions_list)}\n",
    "\n",
    "        self.states_list = list(product(environment.get_states(), agent.get_states()))\n",
    "        self.states_to_idx = {state: idx for idx, state in enumerate(self.states_list)}\n",
    "\n",
    "        self.initialize_random_policy()\n",
    "        \n",
    "    def initialize_random_policy(self):\n",
    "        self.Q = np.random.random((len(self.states_to_idx), len(self.actions_to_idx)))\n",
    "        self.Returns = {(s, a): [] for s, a in product(self.states_to_idx, self.actions_to_idx)}\n",
    "\n",
    "        self.pi = np.random.random((len(self.states_to_idx), len(self.actions_to_idx)))\n",
    "        self.pi = self.pi / np.sum(self.pi, axis=1)[:, None]\n",
    "    \n",
    "    def on_policy_learning(self, num_iterations, epsilon=.1, gamma=1):\n",
    "        \"\"\"\n",
    "            epsilon: sets minimum probability threshold for policy pi\n",
    "            gamma: discount factor in rewards\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        count = 0\n",
    "        learning = True\n",
    "\n",
    "        while learning:\n",
    "\n",
    "            visited_states, actions_taken, rewards = self.agent.get_episode(\n",
    "                self.pi, \n",
    "                self.environment, \n",
    "                self.idx_to_actions, \n",
    "                self.states_to_idx\n",
    "            )\n",
    "\n",
    "            has_visited_first_occurence = {}\n",
    "            for idx, sa in enumerate(zip(visited_states, actions_taken)):\n",
    "                s, a = sa\n",
    "                if (s, a) not in has_visited_first_occurence:\n",
    "                    self.Returns[(s, a)].append(sum(rewards[idx:]))\n",
    "                    self.Q[self.states_to_idx[s], self.actions_to_idx[a]] = np.mean(self.Returns[(s, a)]) \n",
    "                    has_visited_first_occurence[(s, a)] = 0\n",
    "\n",
    "            for s in visited_states:\n",
    "                # We can take the greedy action, but it's probably better to break ties\n",
    "                # a_star = np.argmax(Q[states_to_idx[s],:])\n",
    "                action_idx = np.where(self.Q[self.states_to_idx[s],:] == np.amax(self.Q[self.states_to_idx[s],:]))[0]\n",
    "                a_star = np.random.choice(action_idx)\n",
    "                for action_idx, a in enumerate(self.actions_list):\n",
    "                    if a_star == action_idx:\n",
    "                        self.pi[self.states_to_idx[s], action_idx] = 1 - epsilon + epsilon / len(self.actions_list)\n",
    "                    else:\n",
    "                        self.pi[self.states_to_idx[s], action_idx] = epsilon / len(self.actions_list)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count >= num_iterations: learning = False\n",
    "        \n",
    "        return self.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car = RaceCar()\n",
    "track = Track()\n",
    "\n",
    "actions = list(product([-1, 0, 1], [-1, 0, 1]))\n",
    "mc = MonteCarlo(actions, car, track)\n",
    "\n",
    "# car.get_episode(pi, track, idx_to_actions, states_to_idx, greedy=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi = mc.on_policy_learning(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  3.]\n",
      " [ 1. -1.  1.  0.  0.]\n",
      " [ 2.  2.  2.  0.  0.]]\n",
      "('Velocity is now: ', (0, 1))\n",
      "[[ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1. -1.  1.  3.]\n",
      " [ 1.  1.  1.  0.  0.]\n",
      " [ 2.  2.  2.  0.  0.]]\n",
      "('Velocity is now: ', (1, 1))\n",
      "[[ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  1. -1.]\n",
      " [ 1.  1.  1.  1.  3.]\n",
      " [ 1.  1.  1.  0.  0.]\n",
      " [ 2.  2.  2.  0.  0.]]\n",
      "('Velocity is now: ', (2, 1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([((1, 0), (0, 0)), ((1, 1), (0, 1)), ((2, 2), (1, 1)), ((4.0, 3.0), (2, 1))],\n",
       " [(0, 1), (1, 0), (1, 0)],\n",
       " [-1, -1, -1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.get_episode(pi, track, mc.idx_to_actions, mc.states_to_idx, greedy=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, -1): 0,\n",
       " (-1, 0): 1,\n",
       " (-1, 1): 2,\n",
       " (0, -1): 3,\n",
       " (0, 0): 4,\n",
       " (0, 1): 5,\n",
       " (1, -1): 6,\n",
       " (1, 0): 7,\n",
       " (1, 1): 8}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.actions_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
