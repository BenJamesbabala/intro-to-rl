{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.1   \n",
    "\n",
    "Can you imagine a scenario in which a TD update would be better on average than an Monte Carlo update? \n",
    "\n",
    "As suggested in the hint, if the Value function at a particular state were very accurate due to prior experience, TD updates to the prior states would make the Value functions of the prior states converge quicker to the real Value function. If you do an every-visit Monte Carlo, you would update the value function in a sub-optimal way since $R_t$ is only observed after 1-episode, whereas bootstrapping with Value functions incorporates prior knowledge into the updates.\n",
    "\n",
    "\n",
    "##### Exercise 6.2\n",
    "\n",
    "From Figure  6.6, it appears that the first episode results in a change in only $V(A)$. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?\n",
    "\n",
    "The TD(0) update is $V(s_t) \\leftarrow V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$, so after the first episode, the random walker started at C and terminated immediately after going left past A. The $V(C)$ and $V(B)$ don't change after the first episode since $r_{t+1}$ is 0 and $V(C) - V(B) = .5 - .5 = 0$, etc. However, $V(A)$ is updated as $V(A) = V(A) + \\alpha [0 + \\gamma \\cdot 0 - 0.5] = V(A) - \\alpha \\cdot 0.5$.\n",
    "\n",
    "##### Exercise 6.3\n",
    "\n",
    "Do you think that by choosing the step-size parameter, , differently, either algorithm could have done significantly better than shown in Figure  6.7? Why or why not?\n",
    " \n",
    "The $\\alpha$ parameter specifies the weight updated to the value functions. As $\\alpha$ increases, we expect TD(0) and Every-visit MC to make larger updates to the value functions, and as $\\alpha$ decreases, the updates are smaller. Keeping the number of episodes fixed, the RMSE over states is minimized for some $\\alpha$, but I'm not sure if the $\\alpha$ is unique. If $\\alpha$ were unique, the RMSE in $\\alpha$ may be convex (since $V*$ is a fixed point), and the optimal $\\alpha$ for the MC plot is within the range already searched. For TD(0) it seems like lower $\\alpha < 0.05$ may be more optimal to lower RMSE after 100 episodes.\n",
    "\n",
    "##### Exercise 6.4\n",
    "\n",
    "In Figure  6.7, the RMS error of the TD method seems to go down and then up again, particularly at high $\\alpha$'s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?\n",
    "\n",
    "The TD(0) method with constant $\\alpha$ doesn't actually converge to a fixed point. Equation 2.8 shows us that in order for TD(0) to converge for a stationary environment, $\\sum_{k=1}^{\\infty} \\alpha_k \\rightarrow \\infty$ and $\\sum_{k=1}^{\\infty} \\alpha_k^2 \\lt \\infty$. A constant value for $\\alpha$ fails the second condition, thus irregardless of the initilization of the value function, the RMSE will not converge to a minimum over time and will constantly vary. For small $\\alpha$, it seems like TD(0) does not vary after reaching a minimum within the first 100 episodes in Figure 6.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Exercise 6.5\n",
    "Above we stated that the true values for the random walk task are 1/6, 2/6, 3/6, 4/6, 5/6 for states A through E. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?\n",
    "\n",
    "One way would be to use dynamic programming since the random walk in Figure 6.5 is a finite-state MDP. Another method, is to realize that the task is undiscounted and episodic, so the value of each state is the probability of reaching the terminal state that has reward 1 (since all rewards are 0 except at the right-most terminal state). Therefore it is easy to compute the value of state C, since the problem is symmetrical, the probability of reaching any terminal state is 0.5, so the value is $0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5$. Then we know that $V(D) = 0.5 \\cdot V(E) + 0.5 \\cdot V(C)$ and $V(E) = 0.5 \\cdot 1 + 0.5 \\cdot V(D)$; plugging the $V(E)$ equation into $V(D)$, we get $V(D) = 0.5 (0.5 + 0.5 \\cdot V(D)) + 0.25$, so $V(D) = \\frac{.5}{.75} = \\frac{4}{6}$ and $V(E) = \\frac{5}{6}$. A similar exercise can be done with $V(B)$ and $V(A)$, or by symmetry we can see that $V(A) = 1 - V(E)$ and $V(B) = 1 - V(D)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Exercise 6.6 and 6.7, Windy grid world\n",
    "\n",
    "It takes a lot longer for my SARSA to converge than the one in the book at 8000 time steps with $\\alpha = 0.1$. Looking around on the internet, it seems like the Figure 6.11 may have been done with $\\alpha = 0.5$ (see https://www.cse.iitb.ac.in/~sushantcse/Articles/Analysis_of_SARSA_on_Windy_Gridworld.pdf). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class WindyGridWorld(object):\n",
    "    def __init__(self, grid, wind, stochastic_wind=False):\n",
    "        \"\"\"\n",
    "            1: valid move\n",
    "            2: start\n",
    "            3: end\n",
    "        \"\"\"\n",
    "        self.grid = grid\n",
    "        self.wind = wind\n",
    "        self.wind_is_stochastic = stochastic_wind\n",
    "        self.row = 0\n",
    "        self.column = 0\n",
    "    \n",
    "    def set_starting_state(self):\n",
    "        w = np.where(self.grid == 2)\n",
    "        self.row = w[0][0]\n",
    "        self.column = w[1][0]\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        reward = -1\n",
    "        random_wind = 0\n",
    "\n",
    "        if self.wind_is_stochastic:\n",
    "            random_wind = np.random.choice([-1, 0, 1], size=1, p=[1/3., 1/3., 1/3.])[0]\n",
    "        \n",
    "        # apply action\n",
    "        d_row, d_column = action\n",
    "        new_row = d_row + self.row\n",
    "        new_column = d_column + self.column\n",
    "        if not self.is_out_of_bounds(new_row, new_column):\n",
    "            \n",
    "            # apply wind        \n",
    "            new_row_w_wind = new_row - self.wind[self.column] + random_wind\n",
    "            if not self.is_out_of_bounds(new_row_w_wind, new_column):\n",
    "                new_row = new_row_w_wind\n",
    "            \n",
    "            self.row = new_row\n",
    "            self.column = new_column\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_state(self):\n",
    "        return (self.row, self.column)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return list(product(range(self.grid.shape[0]), range(self.grid.shape[1])))\n",
    "    \n",
    "    def is_terminal_state(self):\n",
    "        if self.grid[self.row, self.column] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_out_of_bounds(self, r, c):\n",
    "        if r < 0 or r > self.grid.shape[0] - 1:\n",
    "            return True\n",
    "        \n",
    "        if c < 0 or c > self.grid.shape[1] - 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_regular_actions(self):\n",
    "        return [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "    \n",
    "    def get_kings_actions(self):\n",
    "        return list(set(product([-1, 0, 1], [-1, 0, 1])) - set([(0, 0)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, environment, actions):\n",
    "        self.environment = environment\n",
    "        self.actions = actions\n",
    "        self.states = environment.get_states()\n",
    "        self.state_idx = {s: idx for idx, s in enumerate(self.states)}\n",
    "        self.Q = np.random.random(size=(len(self.states), len(self.actions)))\n",
    "        \n",
    "    def SARSA(self, max_time_steps=8000, epsilon=0.1, alpha=0.1, gamma=1):\n",
    "        self.Q = np.zeros((len(self.states), len(self.actions)))\n",
    "        \n",
    "        t = 0\n",
    "        episode_history = [0]\n",
    "        while t < max_time_steps:\n",
    "            episode_history.append(episode_history[-1] + 1)\n",
    "            self.environment.set_starting_state()\n",
    "            s = self.environment.get_state()\n",
    "            a = self.get_epsilon_greedy_action(s, epsilon)\n",
    "\n",
    "            while not self.environment.is_terminal_state() and t < max_time_steps:\n",
    "                episode_history.append(episode_history[-1])\n",
    "                r = self.environment.take_action(self.actions[a])\n",
    "                s_prime = self.environment.get_state()\n",
    "                a_prime = self.get_epsilon_greedy_action(s_prime, epsilon)\n",
    "                self.Q[self.state_idx[s], a] += alpha * \\\n",
    "                    (r + gamma * self.Q[self.state_idx[s_prime], a_prime] - self.Q[self.state_idx[s], a])\n",
    "                s = s_prime; a = a_prime\n",
    "                t += 1\n",
    "\n",
    "        return episode_history\n",
    "            \n",
    "    def get_epsilon_greedy_action(self, s, epsilon=0.1):\n",
    "        s_idx = self.state_idx[s]\n",
    "        u_rand = np.random.random()\n",
    "        if u_rand < epsilon:\n",
    "            return np.random.randint(low=0, high=len(self.actions))\n",
    "        action_idx = np.where(self.Q[s_idx,:] == np.amax(self.Q[s_idx,:]))[0]\n",
    "        return np.random.choice(action_idx)\n",
    "    \n",
    "    def get_greedy_episode(self, max_iter=20):\n",
    "        self.environment.set_starting_state()\n",
    "        s = self.environment.get_state()\n",
    "\n",
    "        g_orig = self.environment.grid\n",
    "        g = np.copy(g_orig)\n",
    "        g[s[0], s[1]] = -1\n",
    "        print(g)\n",
    "    \n",
    "        t = 0\n",
    "        while not self.environment.is_terminal_state() and t < max_iter:\n",
    "            a = self.get_epsilon_greedy_action(s, epsilon=0)\n",
    "            print('taking action', a)\n",
    "            r = self.environment.take_action(self.actions[a])\n",
    "            s = self.environment.get_state()\n",
    "            g = np.copy(g_orig)\n",
    "            g[s[0], s[1]] = -1\n",
    "            print(g)\n",
    "            t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = np.array([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [2, 1, 1, 1, 1, 1, 1, 3, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ])\n",
    "wind = np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "max_time_step = 24000\n",
    "# Regular Wind\n",
    "gw = WindyGridWorld(grid, wind, stochastic_wind=False)\n",
    "solver = Solver(gw, gw.get_regular_actions())\n",
    "num_episodes_regular = solver.SARSA(max_time_steps=max_time_step, alpha=0.1)\n",
    "solver = Solver(gw, gw.get_kings_actions())\n",
    "num_episodes_kings = solver.SARSA(max_time_steps=max_time_step, alpha=0.1)\n",
    "\n",
    "# Stochastic Wind\n",
    "gw = WindyGridWorld(grid, wind, stochastic_wind=True)\n",
    "\n",
    "solver = Solver(gw, gw.get_regular_actions())\n",
    "num_episodes_regular_stochastic = solver.SARSA(max_time_steps=max_time_step, alpha=.1)\n",
    "solver = Solver(gw, gw.get_kings_actions())\n",
    "num_episodes_kings_stochastic = solver.SARSA(max_time_steps=max_time_step, alpha=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x113f5ea90>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(num_episodes_regular)\n",
    "plt.plot(num_episodes_kings)\n",
    "plt.plot(num_episodes_regular_stochastic)\n",
    "plt.plot(num_episodes_kings_stochastic)\n",
    "plt.legend(['reg', 'kings', 'reg_stoch', 'kings_stoch'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With alpha = 0.5 and 8000 time steps\n",
    "gw = WindyGridWorld(grid, wind, stochastic_wind=False)\n",
    "solver = Solver(gw, gw.get_regular_actions())\n",
    "num_episodes_regular = solver.SARSA(max_time_steps=8000, alpha=0.5)\n",
    "plt.plot(num_episodes_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.get_greedy_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.8\n",
    "\n",
    "The backup diagram for Sarsa is simply an $(s, a)$ node that gets a reward pointing to another $(s', a')$ node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.9 \n",
    "\n",
    "Q-learning is an off-policy control method since $Q(s,a)$ is not updated based on the actual next state $s'$ and action $a'$ taken by the agent. In Q-learning, $Q(s,a)$ is being updated based on greedy actions that may have not been taken by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.10\n",
    "\n",
    "The proposed algorithm is off-policy since $Q(s,a)$ is not being updated by the next state-action pair that the agent takes, but insead the average over future state-actions. The backup diagram looks similar to Q-learning, except the sweep over next actions is an average. I'd expect this method to work better than Sarsa since the average over multiple future next state-actions is incorporated into the update instead of the state-action that the agent actually  (more information is incorporated into the update)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.11\n",
    "\n",
    "Design an on-policy R-learning method for undiscounted continuing tasks. \n",
    "\n",
    "This would be similar to the off-policy method in Chapter 6.7 since the algorithm shown is also undiscounted and continuing. I'm not really sure about the estimation policy, since in Figure 6.16 it is greedy, whereas here it needs to be on-policy:\n",
    "\n",
    "\n",
    "- Initialize $s$ at current state\n",
    "- Choose $a$ from $s$ using behavioral policy from $Q$\n",
    "- Repeat\n",
    "    - Take $a$ and observe $r, s'$\n",
    "    - Choose $a'$ from $s'$ using behavioral policy from $Q$\n",
    "    - $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r - \\rho + Q(s',a') - Q(s,a)]$\n",
    "    - $\\rho \\leftarrow \\rho + \\beta[r - \\rho + Q(s',a') - Q(s,a)]$ (not sure about this step)\n",
    "    - $a \\leftarrow a'$ and $s \\leftarrow s'$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6.12\n",
    "\n",
    "Describe how the task of Jack's Car Rental (Example 4.2) could be reformulated in terms of afterstates. Why, in terms of this specific task, would such a reformulation be likely to speed convergence?\n",
    "\n",
    "In Jack's Car Rental, the actions deterministically result in the number of cars available for rent the next day (the afterstates). However the states used in DP are the number of cars available at the end of the day. A reformulation using afterstates of Value Iteration to solve Jack's Car Rental may speed up convergence, since the value of the afterstates only depends on the demand distribution and not the actions that Jack took to get to the afterstate. Several states and actions that generate the same afterstates all have the same value, and the demand for cars converges to a known distribution; the $V(s_{after})$ is calculated only off of the demand, rather than Jack's action and then the demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
