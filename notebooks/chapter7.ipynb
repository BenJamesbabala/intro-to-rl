{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 7.1\n",
    "\n",
    "Why do you think a larger random walk task (19 states instead of 5) was used in the examples of this chapter? Would a smaller walk have shifted the advantage to a different value of n? How about the change in left-side outcome from 0 to -1? Would that have made any difference in the best value of n?\n",
    "\n",
    "A small random walk would truncate large n-step to their total returns since episodes will be shorter (i.e. large n would just result in alpha MC methods). Therefore we should expect the advantage at lower n for smaller random walks. \n",
    "\n",
    "With values initialized at 0, if the left-most value terminated in 0 reward, we would need longer episodes for an agent to assign the correct values to the states left of center, since episodes that terminate to the left will not cause any updates initially, only the episodes that terminate to the right end with non-zero reward. Thus I would expect the best value of n to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "##### Exercise 7.2\n",
    "\n",
    "Why do you think on-line methods worked better than off-line methods on the example task?\n",
    "\n",
    "Off-line methods generally take random actions with some small probability $\\epsilon$. We would expect at least 1-2 random actions in an environment with a minimum of 10 states to termination, depending on $\\epsilon$ (assuming $\\epsilon$ is between 10-20%). Therefore, even after finding the optimal action-values, these random actions will attribute erroneous rewards to certain actions, leading to higher RMSEs compared to on-line methods; we also see that larger n is more optimal for off-line methods compared to on-line, presumably because larger n reduces noise from the $\\epsilon$ greedy actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##### Exercise 7.3\n",
    "\n",
    "In the lower part of Figure  7.2, notice that the plot for n=3 is different from the others, dropping to low performance at a much lower value of $\\alpha$ than similar methods. In fact, the same was observed for n=5, n=7, and n=9. Can you explain why this might have been so? In fact, we are not sure ourselves.\n",
    "\n",
    "My hypothesis is that odd values of n have higher RMSE because of the environment. It takes at a minimum, an odd number of steps to reach termination from the starting state. For off-line methods, even after finding the optimal action-values, an agent may still not terminate in an odd number of steps. Therefore my hypothesis is that odd n-step methods are more likely to cause erroneous updates to the $\\epsilon$ greedy actions compared to even n-step methods. A quick way to test this, would be to create a random-walk where an agent will terminate at a minimum in an even number of steps, and then to observe the same plots as in Figure 7.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "#### Exercise 7.4  \n",
    "\n",
    "The parameter $\\lambda $ characterizes how fast the exponential weighting in Figure  7.4 falls off, and thus how far into the future the $\\lambda $-return algorithm looks in determining its backup. But a rate factor such as $\\lambda $ is sometimes an awkward way of characterizing the speed of the decay. For some purposes it is better to specify a time constant, or half-life. What is the equation relating $\\lambda $ and the half-life, $\\tau$, the time by which the weighting sequence will have fallen to half of its initial value?\n",
    "\n",
    "The half life occurs when weighting drops in half:\n",
    "\n",
    "$ \\lambda^{n} = 0.5 $,\n",
    "\n",
    "which occurs at,\n",
    "$n = -ln(2) / ln(\\lambda) = \\tau$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Getting (7.3) from the equation above it:\n",
    "\n",
    "$R_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} R^{(n)}_t$,\n",
    "\n",
    "after $T-t-1$, we sum to infinity but with $R^{T-t-1}_t$, which is just the total return $R_t$, so:\n",
    "\n",
    "$R_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{T-t-1} \\lambda^{n-1} R^{(n)}_t + (1 - \\lambda) R_t \\sum_{n=T-t-1}^{\\infty} \\lambda^{n} $\n",
    "\n",
    "We can remove $\\lambda^{T-t-1}$ from the last sum to get $ (1 - \\lambda) R_t \\lambda^{T-t-1} \\sum_{n=1}^\\infty \\lambda^n = (1 - \\lambda) R_t \\lambda^{T-t-1} \\frac{1}{1 - \\lambda}$, so that: \n",
    "\n",
    "$R_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^{T-t-1} \\lambda^{n} R^{(n)}_t + \\lambda^{T-t-1} R_t  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "##### Exercise 7.5\n",
    "\n",
    "In order to get TD($\\lambda$) to be equivalent to the $\\lambda$-return algorithm in the online case, the proposal is that $\\delta_t = r_{t+1} + \\gamma V_t(s_{t+1}) - V_{t-1}(s_t) $ and the n-step return is $R_t^{(n)} = r_{t+1} + \\dots + \\gamma^{n-1} r_{t+n} + \\gamma^n V_{t+n-1}(s_{t+n}) $. To show that this new TD method is equivalent to the $\\lambda$ return, it suffices to show that $\\Delta V_t(s_t)$ is equivalent to the new TD with modified $\\delta_t$ and $R_t^{(n)}$.\n",
    "\n",
    "As such, we expand the $\\lambda$ return:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{1}{\\alpha} \\Delta V_t(s_t) =&  -V_{t-1}(s_t) + R_t^\\lambda\\\\\n",
    "=& -V_{t-1}(s_t) + (1 - \\lambda) \\lambda^0 [r_{t+1} + \\gamma V_t(s_{t+1})] + (1-\\lambda) \\lambda^1 [r_{t+1} + \\gamma r_{t+2} + \\gamma^2 V_{t+1}(s_{t+2})] + \\dots\\\\\n",
    "=& -V_{t-1}(s_t) + (\\gamma \\lambda)^0 [r_{t+1} + \\gamma V_t(s_{t+1}) - \\gamma \\lambda V_t(s_{t+1})] + (\\gamma \\lambda)^1 [r_{t+2} + \\gamma V_{t+1}(s_{t+2}) - \\gamma \\lambda V_{t+1}(s_{t+2})] + \\dots\\\\\n",
    "=& (\\gamma \\lambda)^0 [r_{t+1} + \\gamma V_t(s_{t+1}) - V_{t-1}(s_t)] + (\\gamma \\lambda) [r_{t+2} + \\gamma V_{t+1}(s_{t+2}) - V_t(s_t+1)] + \\dots\\\\\n",
    "=& \\sum_{k=t}^\\infty (\\gamma \\lambda)^{k-t} \\delta_k\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where $\\delta_k = r_k + \\gamma V_k(s_{k+1}) - V_{k-1}(s_k)$ as defined in the problem. Therefore, for online TD as defined above, the $\\lambda$ return is exactly equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "##### Exercise 7.6\n",
    "\n",
    "  In Example 7.5, suppose from state s the wrong action is taken twice before the right action is taken. If accumulating traces are used, then how big must the trace parameter $\\lambda $ be in order for the wrong action to end up with a larger eligibility trace than the right action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##### Exercise 7.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##### Exercise 7.8\n",
    "\n",
    "sarsa($\\lambda$) with replacing traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "##### Exercise 7.9\n",
    "\n",
    "Write pseudocode for an implementation of TD($\\lambda $) that updates only value estimates for states whose traces are greater than some small positive constant.\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "##### Exercise 7.10\n",
    "\n",
    "Prove that the forward and backward views of off-line TD($\\lambda $) remain equivalent under their new definitions with variable $\\lambda $ given in this section. Follow the example of the proof in Section 7.4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** \"Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks.\"**\n",
    "\n",
    "\"In the future it may be possible to vary the trade-off between TD and Monte Carlo methods more finely by using variable $\\lambda $, but at present it is not clear how this can be done reliably and usefully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
